{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be24cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/May/2024 04:52:20] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:20] \"GET /lib/owlcarousel/assets/owl.carousel.min.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:21] \"GET /static/feature.jpg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:22] \"GET /static/bg.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:22] \"GET /img/overlay-bottom.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:26] \"GET /img/overlay-top.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:26] \"GET /img/favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/May/2024 04:52:26] \"GET /img/bg-image.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/May/2024 04:53:26] \"GET /img/favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://maps.google.com/maps%3Fq%3DThe%2Bsimilarity%2Bscore%2Bis%2Bnow%2Bmultiplied%2Bby%2B100%2Band%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error fetching https://www.scribbr.com/frequently-asked-questions/check-document-multiple-times/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/May/2024 04:56:51] \"POST /check HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DThe%252Bsimilarity%252Bscore%252Bis%252Bnow%252Bmultiplied%252Bby%252B100%252Band%252B%26hl%3Den: 404 Client Error: Not Found for url: https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DThe%252Bsimilarity%252Bscore%252Bis%252Bnow%252Bmultiplied%252Bby%252B100%252Band%252B%26hl%3Den\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric, strip_short\n",
    "import difflib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/check', methods=['POST'])\n",
    "def check_plagiarism():\n",
    "    user_input = request.json.get('text')\n",
    "    if not user_input:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "    \n",
    "    try:\n",
    "        result = check_text_for_plagiarism(user_input)\n",
    "        return jsonify(result)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "def scrape_web(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    results = []\n",
    "    for g in soup.find_all('a'):\n",
    "        link = g.get('href')\n",
    "        if link and 'url?q=' in link:\n",
    "            results.append(link.split('url?q=')[1].split('&')[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        page_content = ' '.join([para.text for para in paragraphs])\n",
    "        return page_content\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    custom_filters = [strip_punctuation, strip_numeric, strip_short]\n",
    "    return preprocess_string(text, custom_filters)\n",
    "\n",
    "def compare_texts(text1, text2):\n",
    "    texts = [preprocess(text1), preprocess(text2)]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    \n",
    "    vec_bow = dictionary.doc2bow(preprocess(text2))\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    \n",
    "    sims = index[vec_lsi]\n",
    "    return float(sims[0])\n",
    "\n",
    "def highlight_text(original, plagiarized):\n",
    "    original_words = original.split()\n",
    "    plagiarized_words = plagiarized.split()\n",
    "    diff = difflib.ndiff(original_words, plagiarized_words)\n",
    "    highlighted = []\n",
    "    \n",
    "    for word in diff:\n",
    "        if word.startswith('- '):\n",
    "            highlighted.append(f\"<span style='color: red;'>{word[2:]}</span>\")\n",
    "        else:\n",
    "            highlighted.append(word[2:])\n",
    "    \n",
    "    return ' '.join(highlighted)\n",
    "\n",
    "def check_text_for_plagiarism(text):\n",
    "    search_results = scrape_web(text[:50])\n",
    "    results = []\n",
    "    for url in search_results:\n",
    "        page_content = fetch_page_content(url)\n",
    "        if page_content:\n",
    "            similarity_score = compare_texts(text, page_content)\n",
    "            if similarity_score > 0:\n",
    "                results.append({\n",
    "                    \"url\": url,\n",
    "                    \"similarity\": similarity_score,\n",
    "                    \"content\": page_content\n",
    "                })\n",
    "    \n",
    "    if results:\n",
    "        best_match = max(results, key=lambda x: x['similarity'])\n",
    "        best_match['similarity'] = round(best_match['similarity'] * 100, 2)  # Convert to percentage\n",
    "        best_match['highlighted_content'] = highlight_text(text, best_match['content'][:500])\n",
    "    else:\n",
    "        best_match = {\"url\": \"\", \"similarity\": 0.0, \"highlighted_content\": \"\", \"content\": \"\"}\n",
    "\n",
    "    return best_match\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4310c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c25fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
